import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Load the dataset
data = pd.read_csv('crop.csv')
print("\nMissing Values:")
print(data.isnull().sum())

# Handle missing values (if any)
# Example: Filling missing numeric values with the median
data.fillna(data.median(numeric_only=True), inplace=True)

# Handle categorical columns (if any)
categorical_cols = data.select_dtypes(include=['object']).columns
for col in categorical_cols:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])

# Feature scaling
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data)
data_scaled = pd.DataFrame(scaled_data, columns=data.columns)

# Splitting the dataset into training and testing sets
# Assuming the last column is the target variable
X = data_scaled.iloc[:, :-1]
y = data_scaled.iloc[:, -1]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

data['SEI'] = (data['N_SOIL'] + data['P_SOIL'] + data['K_SOIL']) * (data['TEMPERATURE'] + data['HUMIDITY'])

# 2. Profitability Score
data['Profitability_Score'] = data['CROP_PRICE'] / (1 + data['SEI'])

# 3. Crop-Location Interaction
data['Crop_Location'] = data['CROP'] + '_' + data['STATE']

# 4. Rainfall to Temperature Ratio (RTR)
data['RTR'] = data['RAINFALL'] / (1 + data['TEMPERATURE'])

# 5. Soil Acidity Groups
def categorize_ph(ph_value):
    if ph_value < 6.5:
        return 'Acidic'
    elif 6.5 <= ph_value <= 7.5:
        return 'Neutral'
    else:
        return 'Alkaline'

data['Soil_Acidity'] = data['ph'].apply(categorize_ph)
def time_series_feature_engineering(data):
    # Ensure the data is sorted by a logical order (e.g., STATE and CROP)
    data = data.sort_values(by=['STATE', 'CROP']).reset_index(drop=True)
    
    # Create rolling features for numerical columns
    rolling_features = ['N_SOIL', 'P_SOIL', 'K_SOIL', 'TEMPERATURE', 'HUMIDITY', 'RAINFALL', 'CROP_PRICE']
    for col in rolling_features:
        data[f'{col}_rolling_mean_3'] = data.groupby(['STATE', 'CROP'])[col].transform(lambda x: x.rolling(3).mean())
        data[f'{col}_rolling_sum_3'] = data.groupby(['STATE', 'CROP'])[col].transform(lambda x: x.rolling(3).sum())
        data[f'{col}_rolling_std_3'] = data.groupby(['STATE', 'CROP'])[col].transform(lambda x: x.rolling(3).std())
    
    # Create lag features
    for col in rolling_features:
        data[f'{col}_lag_1'] = data.groupby(['STATE', 'CROP'])[col].shift(1)
    
    # Create differences between consecutive rows
    for col in rolling_features:
        data[f'{col}_diff_1'] = data.groupby(['STATE', 'CROP'])[col].diff(1)
    
    # Drop rows with NaN values generated by rolling and shifting
    data = data.dropna().reset_index(drop=True)
    return data

# Apply the function to the dataset
crop_data_transformed = time_series_feature_engineering(data)
categorical_features = ['STATE', 'CROP']
numerical_features = ['N_SOIL', 'P_SOIL', 'K_SOIL', 'TEMPERATURE', 'HUMIDITY', 'ph', 'RAINFALL', 'CROP_PRICE']

# Encode categorical features
label_encoders = {}
for col in categorical_features:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])
    label_encoders[col] = le  # Save the encoders if needed later

# Define features (X) and target variable (y)
X = data[numerical_features + ['STATE']]  # Exclude 'CROP' from features
y = data['CROP']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the model
model = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust n_estimators
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Optionally, inspect feature importance
importances = model.feature_importances_
feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': importances})
print("\nFeature Importance:")
print(feature_importance.sort_values(by='Importance', ascending=False))
categorical_features = ['STATE', 'CROP']
numerical_features = ['N_SOIL', 'P_SOIL', 'K_SOIL', 'TEMPERATURE', 'HUMIDITY', 'ph', 'RAINFALL', 'CROP_PRICE']

# Encode categorical features
label_encoders = {}
for col in categorical_features:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])
    label_encoders[col] = le  # Save the encoders if needed later

# Define features (X) and target variable (y)
X = data[numerical_features + ['STATE']]  # Exclude 'CROP' from features
y = data['CROP']

# Define features (X) and target variable (y)
features = ['N_SOIL', 'P_SOIL', 'K_SOIL', 'TEMPERATURE', 'HUMIDITY', 'ph',
       'RAINFALL', 'STATE', 'CROP_PRICE', 'CROP'] 
target = 'CROP' 

X = data[features]
y = data[target]

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

# Visualize results (optional)
# ... (You can use libraries like matplotlib or seaborn to create plots)

# Save the model (optional)
import pickle
with open('crop.pkl', 'wb') as file:
    pickle.dump(model, file)